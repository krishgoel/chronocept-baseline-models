{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import DataLoader, evaluate_model\n",
    "from models import XGBoostRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.dataloader:Initializing DataLoader...\n",
      "INFO:utils.dataloader:Using device: cuda\n",
      "INFO:utils.dataloader:Loading dataset for benchmark benchmark_1 with split None\n",
      "INFO:utils.dataloader:No split specified; loading all splits (train, validation, test).\n",
      "INFO:utils.dataloader:Dataset loaded successfully.\n",
      "INFO:utils.dataloader:Initializing embedding for method bert_cls...\n",
      "INFO:utils.dataloader:Initialized BERT model and tokenizer.\n",
      "INFO:utils.dataloader:Embedding resources initialized.\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(\n",
    "    benchmark=\"benchmark_1\",\n",
    "    split=None,\n",
    "    embedding=\"bert_cls\", \n",
    "    max_len=16,\n",
    "    include_axes=True,\n",
    "    shuffle_axes=False,  \n",
    "    normalization=\"zscore\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.dataloader:Starting preprocessing of dataset(s)...\n",
      "INFO:utils.dataloader:Processing split: train with 878 samples (shuffle_axes=False)...\n",
      "INFO:utils.dataloader:Processing a single dataset split...\n",
      "INFO:utils.dataloader:Combined feature vector shape: torch.Size([878, 6912])\n",
      "INFO:utils.dataloader:Extracting and normalizing target values...\n",
      "INFO:utils.dataloader:Converting target values to logarithmic scale...\n",
      "INFO:utils.dataloader:No conversion needed; log_scale is already 1.1.\n",
      "INFO:utils.dataloader:Normalizing target values...\n",
      "INFO:utils.dataloader:Z-score parameters: mean=[ 5.44271868e+01  1.15348462e+01 -2.48804100e-02], std=[20.1219028   3.81394874  1.39502752]\n",
      "INFO:utils.dataloader:Processing split: validation with 247 samples (shuffle_axes=False)...\n",
      "INFO:utils.dataloader:Processing a single dataset split...\n",
      "INFO:utils.dataloader:Combined feature vector shape: torch.Size([247, 6912])\n",
      "INFO:utils.dataloader:Extracting and normalizing target values...\n",
      "INFO:utils.dataloader:Converting target values to logarithmic scale...\n",
      "INFO:utils.dataloader:No conversion needed; log_scale is already 1.1.\n",
      "INFO:utils.dataloader:Normalizing target values...\n",
      "INFO:utils.dataloader:Z-score parameters: mean=[53.97165992 11.64925101  0.09862348], std=[21.3092882   3.78886061  1.38975187]\n",
      "INFO:utils.dataloader:Processing split: test with 129 samples (shuffle_axes=False)...\n",
      "INFO:utils.dataloader:Processing a single dataset split...\n",
      "INFO:utils.dataloader:Combined feature vector shape: torch.Size([129, 6912])\n",
      "INFO:utils.dataloader:Extracting and normalizing target values...\n",
      "INFO:utils.dataloader:Converting target values to logarithmic scale...\n",
      "INFO:utils.dataloader:No conversion needed; log_scale is already 1.1.\n",
      "INFO:utils.dataloader:Normalizing target values...\n",
      "INFO:utils.dataloader:Z-score parameters: mean=[53.87116279 11.43751938 -0.17321705], std=[20.57259339  3.42227032  1.28830229]\n",
      "INFO:utils.dataloader:Preprocessing of all splits completed.\n"
     ]
    }
   ],
   "source": [
    "data = dl.preprocess()\n",
    "X_train, y_train = data[\"train\"]\n",
    "X_valid, y_valid = data[\"validation\"]\n",
    "X_test, y_test = data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: torch.Size([878, 6912]) (878, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes:\", X_train.shape, y_train.shape)\n",
    "\n",
    "if isinstance(X_train, torch.Tensor):\n",
    "    X_train = X_train.cpu().numpy()\n",
    "if isinstance(X_valid, torch.Tensor):\n",
    "    X_valid = X_valid.cpu().numpy()\n",
    "if isinstance(X_test, torch.Tensor):\n",
    "    X_test = X_test.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.xgboost:XGBoost model built (wrapped in MultiOutputRegressor).\n"
     ]
    }
   ],
   "source": [
    "grid_params = {\n",
    "    \"estimator__n_estimators\": [50, 100],\n",
    "    \"estimator__max_depth\": [3, 5],\n",
    "    \"estimator__learning_rate\": [0.1, 0.01]\n",
    "}\n",
    "default_params = {\n",
    "    \"estimator__n_estimators\": 100,\n",
    "    \"estimator__max_depth\": 3,\n",
    "    \"estimator__learning_rate\": 0.1\n",
    "}\n",
    "xgb_model = XGBoostRegressionModel(params=default_params)\n",
    "xgb_model.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.xgboost:Starting grid search for XGBoost...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.xgboost:Grid search complete.\n",
      "INFO:models.xgboost:Best Hyperparameters: {'estimator__learning_rate': 0.1, 'estimator__max_depth': 3, 'estimator__n_estimators': 50}\n",
      "INFO:models.xgboost:Validation MSE for best XGBoost model: 0.9033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'estimator__learning_rate': 0.1, 'estimator__max_depth': 3, 'estimator__n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "best_model, best_params = xgb_model.grid_search(X_train, y_train, X_valid, y_valid, param_grid=grid_params, cv=5)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance:\n",
      "MSE: 0.8884, MAE: 0.7424, R2: 0.1116, NLL: 1.3598, CRPS: 0.7424, Pearson: 0.3062, Spearman: 0.2940\n"
     ]
    }
   ],
   "source": [
    "test_preds = best_model.model.predict(X_test)\n",
    "mse, mae, r2, nll, crps, pearson_corr, spearman_corr = evaluate_model(y_test, test_preds)\n",
    "print(\"Test Performance:\")\n",
    "print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}, NLL: {nll:.4f}, CRPS: {crps:.4f}, Pearson: {pearson_corr:.4f}, Spearman: {spearman_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:models.xgboost:XGBoost model saved to baseline_weights/benchmark_1/xgboost.pkl\n"
     ]
    }
   ],
   "source": [
    "save_path = \"baseline_weights/benchmark_1/xgboost.pkl\"\n",
    "best_model.save(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
